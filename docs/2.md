# 第二章 可解释性

目前对于可解释性仍未提出一个数学上的定义。我比较喜欢Miller在2017年提出的一个关于可解释性的非数学定义[$ ^3 $](https://christophm.github.io/interpretable-ml-book/#fn3%20class=)是：**可解释性是指人们对模型做出一个决策的原因的理解程度**。另一种说法是：**可解释性是指一个人能够一贯地预测模型结果的程度**[$^4$](https://christophm.github.io/interpretable-ml-book/#fn4%20class=)。总之，机器学习模型的可解释性越高，人类就越容易理解它为什么做出某些决定或预测。如果一个模型的决策比另一个模型的决策能让人更容易理解，那么它的可解释性就比另一个模型更高。在本书后面的内容中将同时使用**Interpretable**(可解释的)和**Explainable**(可解释的)这两个术语<sub>c</sub>。正如同Miller一样$^3$，我认为对术语**Interpretability**及**Explainability**(可解释性)进行区分及解释是有意义的。我将使用**Explanation**来表示对单一样例预测的解释。在2.6节中，可以了解到什么是人类所认为的好的解释。

3. Miller, Tim. "Explanation in artificial intelligence: Insights from the social sciences." arXiv Preprint arXiv:1706.07269. (2017).
4. Kim, Been, Rajiv Khanna, and Oluwasanmi O. Koyejo. "Examples are not enough, learn to criticize! Criticism for interpretability." Advances in Neural Information Processing Systems (2016).

+ 译注c：这两个词都是可解释性的意思，但是在有关机器学习可解释性的研究中，意思略有区别。

---

## 2.1 可解释性的重要性


如果一个机器学习模型的性能表现良好，为什么我们不忽略它做出一个明确决定的**原因**而直接**相信这个模型呢**？主要是因为一个单一指标(如分类准确性)对大多数现实任务来说是不完整的描述[$^5$](https://christophm.github.io/interpretable-ml-book/#fn5%20class=)。

让我们来更深入地探讨一下可解释性为什么这么重要。当进行预测建模时，你必须权衡一下：你是否只想知道预测的结果**是什么**？例如，客户流失的可能性或某些药物对患者的疗效。或者，你想知道模型**为什么**会做出这样预测，以及提高模型可解释性的话是否会以降低模型的预测性能为代价。在某些情况下，你不关心做出决定的原因，只需要知道模型在测试集上的预测性能很好就足够了。但在其他情况下，了解“为什么”可以帮助你对问题、数据以及模型可能失败的原因等信息了解得更多。有些应用在低风险环境中的模型可能不需要解释，因为这些错误不会产生严重的后果(例如电影推荐系统)，或者该方法已经经过大量的研究和评价(例如光学字符识别OCR)。对可解释性的需求来自问题形式化的不完整性$^5$，这意味着对于某些问题或任务，仅获得预测结果(**是什么**)是不够的，模型还必须解释它是如何得出预测的(**为什么**)，因为正确的预测只解决了原来问题的一部分。下面是推动可解释性需求的原因$^{3,5}$。

**人类的知识与好奇心**：当一些出乎意料的事情发生时，人类的环境知识<sub>d</sub>模型就会被更新，是通过为意外事件找到解释(explanation)来进行更新的。例如，一个人突然感到不舒服，他问：“为什么我感到这么不舒服？“。他知道每次吃那些红色浆果他都会生病，于是他在他的知识模型中更新了”浆果导致了疾病，因此应该避免“。当研究中使用不透明的机器学习模型时，如果模型只给出预测而没有给出解释，那么科学发现将仍是完全未知的。为了促进知识的学习，满足关于机器为什么产生这样的预测和行为的好奇心，可解释性是至关重要的。当然，人类不需要对发生的任何事情进行解释。对于大多数人来说，他们可以不理解计算机是如何工作的的，但意外事件仍然使我们好奇。例如：为什么我的计算机会意外关闭？

与知识学习密切相关的是人类**寻找在世界上存在的意义的欲望**。我们希望协调我们知识结构要素之间的矛盾或不一致。“为什么我的狗会咬我，尽管它以前从来没有这样做过？”一个人可能会这样问。我们对于狗过去的行为的知识与新发生的、令人不快的咬伤经历之间存在着矛盾。兽医的解释协调了狗主人的矛盾：“那只狗是因为紧张才咬人的。”机器的决策对人的生活影响越大，机器对它的行为做出解释就越重要。如果机器学习模型拒绝贷款申请，这对于申请者来说可能是完全意外的。他们只能用一些可能的解释来协调期望和现实之间的不一致。这些解释并不需要完全说清楚情况，但应该至少提供一个主要原因。另一个例子是产品推荐算法。就我个人而言，我总是想，为什么某些产品或电影会通过算法被推荐给我。通常情况下这种原因是很清楚的：网络上的广告无处不在，因为我最近买了一台洗衣机，所以我知道在接下来的几天里，我会被洗衣机广告轰炸。如果我的购物车里已经有一顶冬帽，那么推荐我购买手套是合理的是合理的。因为在其他电影上和我有相似的观影爱好的用户也喜欢这部电影，所以算法会把这部电影推荐给我。越来越多的互联网公司在他们的推荐中增加了解释，一个很好的例子就是基于经常购买的产品组合进行推荐：

![img](https://christophm.github.io/interpretable-ml-book/images/amazon-freq-bought-together.png)

<center><font size = 2>图2.1 从亚马逊购买油漆时的推荐产品</font></center>

在许多学科中，研究方法都逐渐从定性方法向定量方法变化(例如社会学、心理学)。此外，也有一些向着机器学习方法变化(如生物学、基因学)。**科学的目标**是获取知识，但是许多问题都是通过大的数据集和黑盒机器学习模型来解决的。知识的来源应该是模型本身，而不是数据。可解释性使得通过模型获取额外知识成为可能。

承担实际任务的机器学习模型需要进行**安全度量与测试**。想象一下，一辆自动驾驶的汽车根据深度学习系统自动检测骑自行车的人，你希望100%地确定系统所学到的抽象知识是没有错误的，因为汽车直接碾过骑车人是非常糟糕的。一种解释可能会通过识别自行车的两个轮子作为最重要的学习特征，这种解释可以帮助你考虑特殊情况，例如自行车的侧袋挡住了车轮的一部分。

在默认情况下，机器学习模型从训练数据中学习到了某种偏见。这可能使你的机器学习模型变成歧视非代表性群体的种族主义者。可解释性是机器学习模型中一种有效**检测偏见**的调试工具。训练好的自动批准或拒绝信用申请的机器学习模型可能会歧视少数曾经被剥夺权利的人。你的主要目标是只向最终会偿还贷款的人提供贷款。在这种情况下，问题表述的不完整性在于，你不仅想要尽可能地减少贷款违约，而且也有需要保证不根据统计特征歧视特定的群体。这是问题表述(以低风险和合规的方式发放贷款)中的一个附加约束，而机器学习模型所需要优化的损失函数中并没有关于这一点的表示。

只有增加机器和算法的**社会认可度**，才能使机器学习模型顺利地应用到到日常生活中，在这一过程中可解释性是必不可少的。人们把信仰、欲望、意图等归因于物体，在一个著名的实验中，Heider和Simmel[$^6$](https://christophm.github.io/interpretable-ml-book/#fn6%20class=)向参与者展示了一些关于图形的视频，其中一个圆圈打开一扇“门”进入一个“房间”(只用简单的矩形来表示)。参与者描述了图形的动作，就像他们描述人的行为一样，他们为图形赋予了思想，甚至情感和性格特征。机器人是一个很好的例子，比如我的扫地机器人，名字叫作“Doge”。如果Doge被卡住了，我想：“Doge 想继续打扫，但因为它卡住了，所以需要向我寻求帮助。”后来，当Doge打扫完并在房间里找插座进行充电时，我想：“Doge想找一个插座来充电。”此时，我就给它赋予了性格特征：“Doge有点笨，但很可爱。”当我发现Doge在尽职地打扫房子的过程中撞倒了一棵植物时，我这样的想法就更强烈。能解释其预测机制的机器或算法会得到更多的认可它，在2.6节中，我们将针对“解释是一个社会化过程”展开讨论。

解释还会被用在**社会互动管理**中。通过共享某个事物的解释意义，解释者影响着接受者的行为、情感和信念。对于一个需要和人类进行交互的机器，它可能需要模拟人类的情感和信念。换言之，机器为了达到预期的目标，就必须“说服”我们。如果我的扫地机器人没有在某种程度上解释它的行为，我就无法彻底地接受它。例如在一起“事故”(又一次被困在浴室地毯上……)中，扫地机器人给出了它的解释意义而不是简单地停止工作却不解释，它解释说它被卡住了。有趣的是，机器解释(创建信任)的目标和接收者的目标(理解预测或行为)之间可能存在偏差。也许对Doge被卡住原因的完整解释可能是电池电量非常低，其中一个轮子工作不正常，或者有一个bug让它一次又一次地走到有障碍物的地方。这些原因(可能还有一些其他的)导致机器人会被卡住，虽然它只解释了有什么东西挡在路上，但这足以让我相信它的行为，并得到了事故的解释意义。顺便说一下，Doge又被困在浴室里了。我们每次都要把地毯拆下来，然后再让Doge清理。

![img](https://christophm.github.io/interpretable-ml-book/images/doge-stuck.jpg)

<center><font size = 2>图2.2 我们的扫地机器人Doge卡住了。作为事故的解释，Doge告诉我们，它需要在一个平面上</font></center>

只有在机器学习模型可解释时，才能**调试和检查**它。即使在低风险环境中，例如电影推荐，在研究和开发阶段以及实际部署之后，模型的可解释能力也是很有价值的。在这些阶段之后，当一个模型用于一个产品时，它可能会出错，对错误预测的解释有助于理解模型出错的原因，这为系统的修复提供了指导方向。思考一个哈士奇与狼分类器的例子，它会将一些哈士奇误认为是狼。使用可解释的机器学习方法，你会发现是由于图像上的雪造成了分类结果出错。分类器学会了将雪作为特征来区分狼和哈士奇，将有雪的图像分类为“狼”，这可能有助于在训练数据集中对狼和哈士奇进行区分，但在实际应用中却没有什么意义。

如果能够确保机器学习模型能够对决策进行解释，你还可以更容易地检验以下特征$^5$：

- 公平性：确保预测是公正的，不隐含或明确地歧视非代表性群体。可解释的模型可以告诉你它为什么判断一个人不应该得到贷款，对于一个人来说，很容易判断这个决定是否是从统计特征(如种族)中学习到的偏见。
- 隐私：确保数据中的敏感信息得到保护。
- 可靠性或稳健性：确保输入中的微小改变不会导致预测发生大的变化。
- 因果关系：检查是否只学习到因果关系。
- 信任：与黑盒模型相比，人们更容易信任可以解释其决策的系统。

**我们什么时候不需要可解释性**

以下场景说明了我们何时不需要甚至不希望了解机器学习模型的可解释性。

如果模型**没有显著影响**，则不需要解释性。想象一下一个叫做Mike的人正在做一个机器学习方面的项目，根据Facebook的数据预测他的朋友们下一个假期会去哪里。Mike只是想通过对数据的学习推测朋友们会去哪里度假，从而给他的朋友们一个惊喜。如果模型预测错了也没有什么问题(最坏的情况是Mike会有点尴尬)。因此，即便Mike不能解释模型的输出，那也没有问题。在这种情况下，没有可解释性是完全可以的。如果Mike想要通过对旅行目的地的预测开展商业活动，那么这种情况就完全不同了。如果模型是错误的，企业可能会赔钱，或者对某些人来说，模型可能会因为存在种族歧视变得更糟。一旦模型对金融或者社会产生重大影响，可解释性就变得有至关重要了。

当某个问题已经被**研究得十分透彻**时，就不需要解释性。在一些已经被深入研究的应用领域，因为有着足够的实践经验，模型的问题已经随着时间的推移得到解决。比如用来从信封的照片上提取地址的光学字符识别机器学习模型，这些系统已经应用多年了，使用经验十分丰富，很显然它们一定是有效的。此外，我们对了解该系统实现从图片中提取文字的原理并不是很感兴趣。

可解释性使人或程序**操控系统**成为可能。由于模型创建者和实际用户之间的目标不一致，导致了可能会存在用户欺骗系统的问题。比如信用评分系统，因为银行希望确保贷款只发放给可能归还贷款的申请人，而即便银行不想贷款给他们，申请人的目的仍然是获得贷款。这两个目标之间的不匹配迫使申请者欺骗系统，以增加他们获得贷款的机会。如果申请人知道拥有两张以上的信用卡会对他的分数产生负面影响，他只要简单地注销第三张信用卡就可以提高他的分数，并在获批贷款后申请新的信用卡。虽然他的分数有所提高，但他实际偿还贷款的可能性并没有发生变化。只有当输入是因果特征的代理<sub>e</sub>，却不是真正直接导致最终结果的原因时，系统才会被欺骗。为了不让系统被欺骗，应该尽可能减少代理特征的使用。例如，Google曾经开发了一个名为“Google流感趋势”的系统来预测流感的爆发。该系统将Google搜索的结果与流感爆发相关联，但其表现并不好。搜索的分布发生了变化，Google流感趋势错过了许多次流感爆发。人们搜索“发烧”这样的症状仅仅与实际流感爆发存在一定的相关性，但Google搜索并不会引起流感。理想情况下，模型只使用因果特征，因为它们不会被欺骗。

5. Doshi-Velez, Finale, and Been Kim. "Towards a rigorous science of interpretable machine learning," no. Ml: 1–13. http://arxiv.org/abs/1702.08608 ( 2017).

6. Heider, Fritz, and Marianne Simmel. "An experimental study of apparent behavior." The American Journal of Psychology 57 (2). JSTOR: 243–59. (1944).

* 译注d：机器学习中所说的知识一般指的是一种认知，或是一种客观事实的陈述，比如，译者是一名在读的研究生。
* 译注e：这里所说的代理特征可以理解为间接特征。

---

## 2.2 解释方法分类

机器学习可解释性的方法可以根据不同的标准进行分类。

**自解释还是事后解释**?根据可解释性的实现方式，是通过限制机器学习模型的复杂度(自解释)，还是通过应用分析训练后模型的方法进行解释(事后解释)，可以对可解释方法进行分类。自解释性是指由于自身结构简单而被认为是可解释的机器学习模型所具备的性质，如短决策树或稀疏线性模型等。事后解释性是指在模型训练后运用解释方法对模型进行解释，例如，一种非常重要的事后解释方法就是特征重要性排序。事后解释方法也可以应用于自解释模型上。例如，可以对决策树使用特征重要性排序方法。本书各章节也是根据[自解释](https://christophm.github.io/interpretable-ml-book/simple.html#simple)和[事后(模型无关)解释方法](https://christophm.github.io/interpretable-ml-book/agnostic.html#agnostic)分类进行组织的。

**解释方法的结果**。可以根据不同的解释方法的结果进行粗略的分类。

- **特征概要统计**：许多解释方法可以为每个特征提供概要统计。有些方法为每个特征返回一个数字，例如特征重要性，也可能是更复杂的结果，例如成对特征交互强度，其中包含表示每个特征对交互强度的数字。
- **特征概要可视化**：大多数特征概要统计信息也可以可视化。有些特征摘要实际上只有在可视化的情况下才有意义，只用表格进行解释是一种错误的选择。特征的部分依赖就是这样一种情况。部分依赖图是显示了特征和平均预测结果的曲线。呈现部分依赖关系最好的方法是绘制曲线，而不是输出坐标。

- **模型内部结构(例如学习到的权重)**：对自解释模型的解释就属于这一类。如线性模型中的权重或决策树学习到的树结构(用于划分类的特征和阈值)。模型概要统计和模型内部结构之间没有明显的界限，例如线性模型内部的权重既属于特征概要统计，也属于模型内部结构。另一种输出模型内部结构的方法是可视化在卷积神经网络中学习到的特征检测器。输出模型内部结构的可解释性方法是模型相关的(见下一个标准)。
- **数据点**：所有返回数据点(已经存在的或新创建的)以使模型可解释的方法都属于这一类。其中一种方法叫做反事实解释。为了解释对一个数据实例的预测，该方法通过用一些方式改变某些特征以改变预测结果(例如预测类别的翻转)，找到相似的数据点。另一个例子是识别预测类的原型。为了发挥作用，输出新数据点的解释方法要求可以解释数据点本身。这对图像和文本很有效，但对于具有数百个特性的表格数据收效甚微。
- **自解释模型**：解释黑盒模型的一个解决方案是用可解释模型(在全局或局部范围)对其进行模拟。可解释模型可以通过查看内部模型参数或特征摘要统计信息来解释它本身。

**模型相关还是模型无关**？模型相关的解释工具仅限于特定的模型类。对于线性模型中回归权重的解释就是模型相关的解释，因为从定义上讲，自解释模型的解释总是模型相关的。仅用于解释如神经网络的工具是模型相关的。模型无关的工具可以应用于任何机器学习模型，并且它是在训练后(事后解释)提供解释的。这些模型无关的方法通常通过分析特征输入和输出对来工作。根据定义，这些方法不能访问模型内部信息，如权重或结构信息。

**局部还是全局**？解释方法是解释单一样例的预测还是整个模型的行为？还是范围介于两者之间？在下一节中介绍关于范围标准的更多信息。

---

## 2.3 可解释性的作用范围

算法通过训练一个模型来产生对样例的预测，其中的每个步骤都可以利用透明度或可解释性进行评估。

### 2.3.1 算法透明度

*算法是如何创建模型的？*

透明度是关于算法如何从数据中学习模型，以及它可以学习到什么关系的一种度量。如果使用卷积神经网络对图像进行分类，可以解释为该算法在最底层学习边缘检测算子和过滤器。这是对算法如何工作的理解，但既不是对最终学习到的特定模型的解释，也不是对如何做出单一样例预测的解释。算法的透明性只需要了解算法，而不需要了解数据或学习到的模型。这本书的重点是模型的可解释性，而不是算法的透明度。很多透明度较高的算法，如线性模型的最小二乘法，已被深入地研究和理解。被普遍认为透明度很低的深度学习方法(通常通过包含数以百万计的权重的网络推动梯度的变化)理解起来就没那么容易了，对其内部工作机制的探索是目前领域内研究的重点。

### 2.3.2  全局的模型可解释性

*训练好的模型如何进行预测？*

一旦你能理解整个模型，你可以用可解释的方式来描述模型$^7$。要解释全局模型输出，你需要训练好的模型、算法知识和数据。这种水平的可解释性是基于在全局角度上对模型特征和每个学习到的组件(如权重、其他参数和结构)的认知来理解模型是如何做出决策的。哪些特征很重要，以及它们之间有什么样的相互作用？全局的模型可解释性有助于通过特征理解目标结果的分布，但在实践中全局的模型可解释性是很难实现的。任何参数或权重较多的模型都是普通人类的短期记忆所不能承受的。我认为你很难真正想象一个具有5个特征的线性模型，因为这意味着要用想象力在5维空间中绘制一个超平面，任何超过3维的特征空间都是人类无法想象的。通常，当人们试图理解一个模型时，他们只会考虑其中的一部分，例如线性模型中的权重。

### 2.3.3 组件级的全局模型可解释性

*模型的某些部分对预测有什么影响？*

具有数百个特征的朴素贝叶斯模型对你我来说都太大了，以至于我们无法完全记住它。即使我们能够记住所有的权重，我们也无法快速对新的数据点做出预测。此外，你需要在大脑中得到所有特征的联合分布，以估计每个特征的重要性以及特征如何它对预测影响的重要性，这是一项不可能的任务。但是你能很容易地理解一个权重。虽然全局的模型可解释性通常是难以实现的，但在组件级理解一些模型是可能的。并非所有模型都在参数级上是可解释的，对于线性模型，可解释部分是权重；对于树来说，是分支(选定的特征加上截止点)和叶子节点预测。例如，线性模型在组件级上看起来似乎可以完美地解释，但单个权重的解释与所有其他权重是相互关联的。对单个权重的解释常常需要使其他输入特征保持相同的值，但这在许多实际应用中并不现实。一个预测房屋价格的线性模型，会同时考虑房子的大小和房间的数量，可能有些房间特征的权重为负值。因为还有与价格高度相关的房间大小这个特征，所以这种情况是可能发生的。在一个人们更喜欢大房间的市场中，如果两个房间的大小相同的话，房间少的房子比房间多的房子更值钱。权重只有在与模型中上下文中的其他特征相关联时才有意义。但是，线性模型中的权重仍然有比深度神经网络中的权重更好的可解释性。

### 2.3.4 单一预测的局部可解释性

*为什么模型会对一个样例做出某种预测？*

你可以着眼于一个样例，检查模型针对这个样例做出了什么样的预测，并解释其原因。如果你关注一个单独的预测，那么这个复杂模型的行为可能更容易理解。在局部，预测可能只依赖于线性或单调的某些特征，而不是对它们有复杂的依赖性。例如，房子的价格可能与它的大小成非线性关系，但是如果你只关注一个给定的 100 平方米的房子，那么在该数据子集中，模型预测的预测结果可能与房子大小成线性关系。你可以通过模拟当你增加或减少 10 平方米的大小时，预测的价格是如何变化的来探索这一点。因此，局部解释比全局解释更准确。在第5章中会介绍一些可以使单一预测更容易解释的方法。

### 2.3.5 一组预测的局部可解释性

*为什么模型对一组样例做出了特定的预测？*

多个样例的模型预测可以通过全局的模型解释方法(组件级)或对单一样例的解释进行解释。全局方法可以通过获取一组样例，将其视为完整的数据集，然后在这个子集上应用全局方法。可以对每个样例使用单一预测的解释方法，然后为子集中的全部样例列出结果或对结果进行汇总。

7. Lipton, Zachary C. "The mythos of model interpretability." arXiv preprint arXiv:1606.03490, (2016).

---

## 2.4 可解释性评估

关于机器学习的可解释性究竟是什么，在研究领域内至今也没有达成共识，甚至连如何衡量可解释性也没有明确的方法。但已经有了一些初步的研究，也有研究者尝试着规范一些评估方法，这些会在后面的章节进行介绍。

Doshi Velez和Kim$^5$提出了三个评价可解释性的主要标准：

**应用层评估(实际任务)**：将解释加入产品中，由最终用户进行测试。考虑一个有机器学习组件的骨折检测软件，它可以定位和标记 X 光片中的骨折点。在应用层，放射科医生直接通过测试骨折检测软件来评估模型。这需要一个良好的实验装置以及对如何评估质量的正确理解。一个很好的基线标准是人类在解释同一个决策时的表现。

**人员层评估(简单任务)**是一个简化的应用层评估。不同的是，这些实验不是由领域专家进行的，而是由普通人进行的。这使得实验更便宜(尤其是当领域专家是放射科医生时)，并且更容易找到更多的测试人员。一个例子是向用户展示不同的解释，让其选择最好的那个。

**功能层评估(代理任务)**不需要人工进行。当使用的这类模型已经在人员层进行了评估时，这是最有效的。例如，可能知道最终用户了解决策树。在这种情况下，解释的质量可以由评估树的深度作为代替。树越短，可解释性得分越高。增加预测性能这一约束条件是有必要的，在提高可解释性的同时要尽量保证树的预测性能保持良好且不会降低太多，即使它对比的对象是较深的树。

下一章重点关注在功能层上对单个预测的解释进行评估。我们将在评估中考虑解释的相关性质是什么。

---

## 2.5 解释的性质

我们的目的是解释机器学习模型的预测。为了实现这一点，要依赖于某些解释方法，这是一种生成解释的算法。**一个以人类可理解的方式给出的关于模型预测的解释通常是与样例的特征值相关的**。其他类型的解释是包括一组数据样例(例如k-最近邻模型的样例)的。例如，我们可以使用支持向量机预测癌症风险并通过[局部代理方法](https://christophm.github.io/interpretable-ml-book/lime.html#lime)来解释预测的结果，并使用生成决策树作为解释。或者我们可以用线性回归模型代替支持向量机，因为线性回归模型本身就具备了解释方法(可以通过权重进行解释)。

Robnik-Sikonja和Bohanec更详细地研究了解释方法及解释的性质$^8$。这些性质可用于判断解释方法或解释的好坏。如何正确地衡量这些性质尚没有一种明确的方法，因此其中一个挑战是如何规范化这些性质的计算方式。

**解释方法的性质**

- **表达能力**是该方法能够生成的解释的“语言”或结构。解释方法可以生成IF-THEN规则、决策树、加权和、自然语言或其他方式的解释。

- **半透明度**描述了解释方法对参考机器学习模型(如参数)的依赖程度。例如，依赖于如线性回归模型(模型相关)这类自解释模型的解释方法是高度半透明的。只依赖于控制输入和观察预测的方法其半透明度为零。根据具体情况，可能需要不同的半透明度。高半透明度的优点是该方法可以依赖更多的信息来生成解释。低半透明度的优点是解释方法更易于移植。

- **可移植性**描述了解释方法可应用于机器学习模型的范围。低半透明度的方法将机器学习模型视为黑盒，具有较高的可移植性。代理模型可能是具有最高可移植性的解释方法。仅适用于特定模型(如循环神经网络)的方法的可移植性较低。

- **算法复杂度**描述了生成解释的方法的计算复杂度。当计算时间成为生成解释的瓶颈时，必须考虑此属性。

**单一解释的性质**

- **准确度**：用解释去预测未知数据的结果会如何？如果将解释用于预测而不是对机器学习模型做解释，那么较高的准确度就十分重要了。如果机器学习模型的准确度也很低，并且目标是解释黑盒模型的作用，那么低准确度就很好了。在这种情况下，忠诚度才是最重要的。
- **忠诚度**：解释与黑盒模型的预测有多接近？高忠诚度是解释的重要性质之一，因为低忠诚度的解释对解释机器学习模型是无用的。准确度和忠诚度密切相关，如果黑盒模型具有较高的准确度并且解释具有较高的忠诚度，那么解释也会有较高的准确度。一些解释只提供局部忠诚度，这意味着解释仅与数据子集(例如[局部代理模型](https://christophm.github.io/interpretable-ml-book/lime.html#lime))甚至只针对单一数据样例(例如[Shapley 值](https://christophm.github.io/interpretable-ml-book/shapley.html#shapley))是相近的。
- **一致性**：经过目标预测类似的相同任务训练的不同模型之间，解释的差别有多大？例如，我在同一个任务上训练一个支持向量机和一个线性回归模型，两者产生了非常相似的预测。利用选定的方法生成解释，并分析针对两种方法生成的解释之间的差别。如果解释非常相似，说明这种解释方法的一致性很高。这个性质的判定有时候会比较困难，因为这两个模型可以使用不同的特征，但得到相似的预测(也叫作”[罗生门效应](https://en.wikipedia.org/wiki/Rashomon_effect)“)。在这种情况下，反而更希望得到较低的一致性，因为此时解释必须非常不一致的。如果模型确实依赖于相似的关系，则需要高一致性。
- **稳定性**：对相似的样例的解释有多相似？一致性比较的是模型之间的解释，稳定性比较的是对特定模型的相似样例之间的解释。高稳定性意味着样例特征的细微变化不会很大程度地改变解释(除非这些细微变化也会强烈地改变预测)。缺乏稳定性可能是因为解释方法的方差较大。换句话说，解释方法会受到需要被解释的样例的特征值的微小变化的强烈影响。缺乏稳定性也可能是由解释方法的非确定性组件引起的，如[局部代理方法](https://christophm.github.io/interpretable-ml-book/lime.html#lime)中的数据采样步骤一样。高稳定性总是可取的。
- **可理解性**：人类对解释的理解程度如何？在这些性质中，可理解性就像是房间里的大象。虽然很难定义和衡量，但非常重要。一个许多人都同意的观点是可理解性取决于解释的受众。衡量可理解性的想法包括测量解释的大小(如线性模型中具有非零权重的特征的数量，决策规则的数量等)或测试人们如何从解释中预测机器学习模型的行为。解释中所用到的特征的可理解性也是应该被考虑的，特征的复杂转换可能比原始特征更难理解。
- **确定性**：解释是否反映了机器学习模型的确定性？许多机器学习模型只给出预测，而没有关于预测正确的模型置信度的描述，也就是说没有说明这样的预测正确的概率。如果模型预测一个病人患癌症的概率为4%，那么另一个具有不同特征值的病人患癌症的概率是否与这个4%的确定性一致？一个包含模型确定性的解释是非常有用的。
- **重要程度**：解释对特征或者部分解释的重要性的反映程度如何？例如，如果由生成决策规则作为单一样例的解释，那么是否清楚该规则的哪些条件最重要？
- **新颖性**：解释是否反映了待解释的数据样例来自远离训练数据分布的“新”区域？在这种情况下，模型可能不准确，解释可能毫无用处。新颖性的概念与确定性的概念是相关的。由于缺乏数据，新颖性越高，模型的确定性就越低。
- **代表性**：一个解释包括多少个样例？解释可以覆盖整个模型(例如线性回归模型中的权重解释)，也可以只表示单一预测(例如[Shapley 值](https://christophm.github.io/interpretable-ml-book/shapley.html#shapley))。

8. Robnik-Sikonja, Marko, and Marko Bohanec. "Perturbation-based explanations of prediction models." Human and Machine Learning. Springer, Cham. 159-175. (2018).

---

## 2.6 人性化解释

让我们更深入地探究可解释机器学习的含义及人类认为“好”的解释是什么，人文科学的研究对我们寻找这个问题的答案会有帮助。Miller$^3$对有关解释的著作进行了大量的调查研究，这一节就是基于他的总结展开的。

在本节中，我想让你相信以下几点：对于一个事件，人类更喜欢将当前情况与不会发生的情况进行对比的简短解释(只有1或2个原因)，异常原因通常会提供很好的解释。解释是解释者与解释受众(接受解释者)之间的社会化互动，因此社会化语义对解释的实际内容有很大的影响。

当你需要一个特定的预测或行为的所有因素的解释时，你不需要一个人性化解释，而是一个因果关系解释。如果法律要求指定全部有影响的特征或者当你在调试机器学习模型时，则可能需要因果归因。在这种情况下，可以忽略以下几点。但在所有其他情况下，当外行人或时间很少的人作为解释的受众时，下面的部分应该是很有用的。

### 2.6.1 什么是解释？

解释是“为什么”这个问题的答案$^3$。

* 为什么治疗不起作用？

* 为什么我的贷款请求被拒绝了？

* 为什么外星人还没有联系我们？

前两个问题可以用“日常”的解释来回答，而第三个问题的答案则来自“更普遍的科学现象及哲学问题”。我们重点关注“日常”类型的解释，因为这些解释是与可解释机器学习相关的。以“如何”提问的问题通常可以改为“为什么”问题：“我的贷款请求是如何被拒绝的？”可以变成“为什么我的贷款请求被拒绝了？”。

在下文中，“解释”一词是指解释的社会化和认知的过程，也指这些过程的产物。解释者可以是人，也可以是机器。

### 2.6.2 什么是好的解释？

本节对Miller关于“好”解释的总结$^3$进行了进一步的提炼，并为可解释机器学习赋予了具体的意义。

**解释具有对比性**$^9$，人类通常不会问为什么会做出某种预测，但会问为什么会做出这种预测而不是另一种预测。我们倾向于在反事实的情况下思考，即“如果输入 x 不同，预测会是怎样的？”对于房价预测，业主可能会想知道为什么预测价格比他们预期的价格高。如果我的贷款申请被拒绝，我不想听到所有的正因素或负因素而只对申请中需要更改的点感兴趣，我想知道我的申请和可通过的申请版本之间的对比。认识到对比解释的重要性是可解释机器学习的一个重要发现。从大多数可解释的模型中，你可以提取一个隐含着将样例的预测与人工数据样例的预测或样例的平均值进行对比的解释。医生可能会问：“为什么这种药对我的病人不起作用？”。他们可能需要一种将他们的患者与被该药物治愈的患者以及与药物不起作用的患者进行对比的解释。对比解释比完整解释更容易理解。医生对药物为什么不起作用这个问题的完整解释可能包括：病人已经有10年的疾病，有11个基因过度表达，病人身体很快将药物分解成无效的化学物质等等。对比解释可能更简洁：与有效治愈的患者相比，药物无效的患者具有使药物疗效降低的基因组合。最好的解释是强调关注对象和参考对象之间最大的差异。

**对比性对于可解释机器学习意味着什么**：人类不需要一个对预测的完整解释，但是想要与另一个样例(可以是人工样例)的预测进行比较。对比解释的创建需要一个参考点来进行比较，这是面向应用的，可能取决于要解释的数据点，也可能取决于接收解释的用户。一个房价预测网站的用户可能需要一个与他们自己的房子或网站上的另一个房子或附近的一个普通房子的房价预测进行对比的解释。自动创建对比解释的解决方案还可能涉及在数据中查找原型。

**解释的可选择性**，人们不希望对事件的所有实际原因进行解释，我们更习惯于从各种可能的原因中选择一个或两个原因作为解释。比如，打开电视新闻：“股票价格的下跌被归咎于最近的软件更新存在的问题导致人们对该公司产品越来越强烈的抵制。”

“Tsubasa和他的球队因为防守薄弱而输掉了比赛：他们给对手太多发挥战略的空间。”

“对机关单位和政府越来越不信任是降低选民投票率的主要因素。”

一个事件可以由各种原因解释的事实被称为罗生门效应(Rashomon Effect)。罗生门是一部日本电影，讲述了一个武士死亡的另类、矛盾的故事(解释)。从不同的特征得到好的预测对于机器学习模型来说是有益的。将多个利用不同特征(不同解释)的模型结合在一起的集成方法通常表现良好，因为平衡所有这些“故事”可以使预测更加稳定和准确。但这也意味着对于某种预测做出的原因有不止一个可以选择的解释。

**可选择性对可解释机器学习意味着什么**：解释要尽可能得简短，即使真实情况很复杂，但只给出1到3个原因。局部可理解的模型无关解释方法在这一点上表现得很好。

**解释的社会性**。解释是解释者和解释受众之间对话或互动的一部分。社会语义决定了解释的内容和本质。如果我想向技术人员解释为什么数字加密货币价值如此之高，我会这样说：“数字货币是去中心化的、分布式的，这种基于区块链的账本不会被一个中央实体控制，对于想确保自己财富安全的人来说，这正是他们所需要的。”但对我于的祖母，我会说：“看，奶奶，加密货币有点像电脑黄金。人们很喜欢买黄金，年轻人就很喜欢买电脑黄金。”

**社会性对可解释机器学习意味着什么**：使得研究者注意机器学习应用的社会环境和目标受众，如何正确使地表述机器学习模型的社会性完全取决于具体的应用，可以找人文学科的专家(如心理学家和社会学家)来帮助你。

**解释更关注异常点**。对于事件的解释，人们通常更关注异常原因$^{10}$。这些都是发生概率很小但仍然发生了的原因，消除这些异常原因将大大改变结果(反事实解释)。人类认为这些“异常”原因是很好的解释，就像Trumbelj和Kononenko曾经举过的一个例子$^{11}$是：假设在教师和学生之间有一个测试情况数据集。学生们在提交一个报告之后可以直接通过他们所选的课程。老师可以额外选择通过问学生问题的方式来测试学生的知识，回答不上这些问题的学生将不及格。学生考前有不同程度的复习，这就决定了学生正确回答老师问题的概率也不同(如果老师决定测试这个学生)。我们要预测一个学生是否会通过这门课，并解释我们的预测。如果老师没有提出任何额外的问题，通过的概率是100%，否则通过的概率取决于学生的准备情况和正确回答问题的概率。

情景 1：老师会经常向学生提出额外的问题(例如，100次中有95次)。一个没有学习的学生(通过问题部分的概率只有10%)不幸被提问了额外的问题，但他没有答对。学生为什么不及格？只能说不学习是学生的错。

情景 2：教师很少问额外的问题(例如，100次中有2次)。对于一个没有复习过的学生来说，我们预测他通过课程的可能性很高，因为提额外问题的可能性很低。当然，其中一个学生没有准备这些问题，他通过这些额外问题测试的概率仍然只有10%。他很不走运，没有回答上老师提出的额外问题，结果他没能通过这门课。失败的原因是什么？我认为现在更好的解释是“因为老师测试了学生”。老师不太可能考试，所以老师表现异常。

**异常对于可解释机器学习意味着什么**：如果一个预测的输入特征在任何意义上都是异常的(比如分类特征的一个罕见类别)，并且该特征影响了预测，那么在解释中应该包含这个异常，即使它和其他“正常”特征对预测的影响相同。在我们的房价预测例子中，一个异常的特征可能是一个有两个阳台的房子价格非常昂贵。即使某些归因方法发现，两个阳台这一点对于价格的影响程度与上述的平均住房面积、良好的邻里环境或最近的装修等特征一样大，但“两个阳台”这一异常特征可能是为什么房子如此昂贵的最好解释。

**解释的真实性**。事实证明，好的解释是真实的(在其他情况下也是如此)。但令人不安的是，这并不是“好”解释的最重要因素。例如，选择性似乎比真实性更重要。仅选择一个或两个可能原因的解释很少涵盖所有的相关原因，它选择性地忽略了部分事实。例如，股市的崩溃不可能是由于一个或两个原因导致的，而应该是有数百万个原因影响了数百万人的行为，最终导致了股市崩溃。

**真实性对可解释机器学习意味着什么**：解释应该尽可能真实地预测事件，在机器学习中有时被称为**保真度**。所以如果我们认为第二个阳台增加了一套房子的价格，那么这也应该适用于其他的房子(或者至少对类似的房子是适用的)。对人类来说，解释的保真度不如它的选择性、对比性和社会性重要。

**好的解释与解释受众的先验知识是一致的**。人类倾向于忽略与他们先验知识不一致的信息，这种效应被称为确认偏差$^{12}$。这种偏差在解释中也是不能避免的，人们往往会贬低或忽略与他们的先验知识不一致的解释。先验知识一般是因人而异的，但也有群体认知的先验知识，如政治世界观。

**一致性对可解释机器学习意味着什么**：良好的解释应该与先验知识是一致的，但这很难整合到机器学习中，并且可能会大幅降低预测性能。关于房屋大小对预测价格的影响，我们的先验知识是：房屋越大，价格越高。假设一个模型显示了房屋大小对某些房屋的预测价格具有负因素。模型之所以学习到这一点，是因为它可以提高预测性能(由于一些复杂的交互作用)，但这种表现与我们的先验知识严重不符。你可以给模型施加一个强制性的单调约束(一个特征只能影响一个方向的预测)，或者使用具有这种性质的模型，如线性模型。

**好的解释具有普遍性**。一个可以解释许多事件的原因是非常普遍的，可以被认为是一个很好的解释。请注意，这与异常原因能够作为良好解释的说法是相矛盾的。异常原因常常比一般原因更重要。根据定义，在给定的情况下，异常原因是很少发生的。在没有异常事件的情况下，一般的解释可以认为是一个很好的解释。还需要注意的是，人们往往会误判联合事件的概率(Joe是图书管理员。他更可能是一个害羞的人还是一个喜欢读书的害羞的人？)。比如，“因为房子很大所以它的价格很昂贵”，这是关于房价的一个非常普遍的，很好的解释。

**普遍性对可解释机器学习意味着什么**：普遍性可以很容易地通过支持特征的数目来衡量，即解释所涉及到的样例数除以样例总数。

9. Lipton, Peter. "Contrastive explanation." Royal Institute of Philosophy Supplements 27 (1990): 247-266.
10. Kahneman, Daniel, and Amos Tversky. "The Simulation Heuristic." Stanford Univ CA Dept of Psychology. (1981).
11. Štrumbelj, Erik, and Igor Kononenko. "A general method for visualizing and explaining black-box regression models." In International Conference on Adaptive and Natural Computing Algorithms, 21–30. Springer. (2011).
12. Nickerson, Raymond S. "Confirmation Bias: A ubiquitous phenomenon in many guises." Review of General Psychology 2 (2). Educational Publishing Foundation: 175. (1998).