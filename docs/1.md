# 第一章 引言

这本书为你讲解如何让(有监督)机器学习模型变得可解释。这些章节中包含一些数学公式，但即使没有公式，你也应该能够理解这些方法背后的思想。这本书不适合那些机器学习的初学者。如果你对机器学习不熟悉，有很多书和其他资源可以学习基础知识。我推荐从Hastie等人所著的《统计学习基础》[$^1$](https://christophm.github.io/interpretable-ml-book/intro.html#fn1)一书以及Andrew发布在线上学习平台coursera.com的[机器学习在线课程](https://www.coursera.org/learn/machine-learning)开始学习机器学习。这本书和这门课程都是免费的！

用于解释机器学习模型的新方法被领域内的学者以惊人的速度发表。要跟上发表的所有文章会使人发疯，几乎不可能。因此，你在这本书中找不到最新颖和最奇特的方法，但是本书构建了机器学习可解释性的方法和基本概念。这些基础知识是为你之后学习如何使机器学习模型可解释做准备。学会这些基本概念后，可以使你能够更好地理解和评估任何在你开始阅读这本书的 5 分钟之后发表在[arxiv.org](https://arxiv.org/)上的最新的关于可解释性的文章(我可能夸大了文章发表的速度)。

这本书以一些不需要理解本书的(反乌托邦)[小故事](https://christophm.github.io/interpretable-ml-book/storytime.html#storytime)开始，希望能使你开心和并令你思考。然后这本书开始探讨[机器学习解释性](https://christophm.github.io/interpretable-ml-book/interpretability.html#interpretability)的一些概念。我们将讨论什么时候可解释性是重要的以及目前有哪些不同类型的解释。全书中使用的术语都可以在1.3节中查找。大多数模型和解释方法都是用真实地数据样例来展示的，这些数据的介绍都可以在第3章中找到。使机器学习可解释的一种方法是使用[自解释模型](https://christophm.github.io/interpretable-ml-book/simple.html#simple)，例如线性模型或决策树等。另一种方法是使用[模型无关的解释工具](https://christophm.github.io/interpretable-ml-book/agnostic.html#agnostic)，这些工具可以应用于任何有监督机器学习模型。第5章介绍了部分依赖图和特征重要性排序等方法。模型无关方法的工作原理是改变机器学习模型的输入，然后计算模型预测输出的变化。返回数据实例作为解释的模型无关方法将在第6章展开讨论。所有的模型无关方法都可以根据它们是在所有数据实例上对全局模型行为进行解释或对单一样例预测进行解释来进一步区分。[部分依赖图](https://christophm.github.io/interpretable-ml-book/pdp.html#pdp)、[累积局部效应](https://christophm.github.io/interpretable-ml-book/ale.html#ale)、[特征交互](https://christophm.github.io/interpretable-ml-book/interaction.html#interaction)、[特征重要性](https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance)、[全局代理模型](https://christophm.github.io/interpretable-ml-book/global.html#global)和[原型与批评](https://christophm.github.io/interpretable-ml-book/proto.html#proto)等方法是针对全局模型行为进行解释的。如果需要对单一样例预测进行解释，可以采用[局部代理模型](https://christophm.github.io/interpretable-ml-book/lime.html#lime)、[基于沙普利值的解释方法](https://christophm.github.io/interpretable-ml-book/shapley.html#shapley)、[反事实解释](https://christophm.github.io/interpretable-ml-book/counterfactual.html#counterfactual)(或与其密切相关的[对抗样本解释](https://christophm.github.io/interpretable-ml-book/adversarial.html#adversarial)方法)等解释方法。此外，还有一些方法可用于解释全局模型行为和单一样例预测的两个方面，如：[个体条件期望](https://christophm.github.io/interpretable-ml-book/ice.html#ice)与[有影响力的实例分析](https://christophm.github.io/interpretable-ml-book/influential.html#influential)等。

本书末尾对[未来可解释的机器学习](https://christophm.github.io/interpretable-ml-book/future.html#future)可能出现的乐观前景进行了展望。

你可以从头到尾阅读这本书，也可以直接跳到你感兴趣的方法上。

我希望你会喜欢这本书！



1. Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. "The elements of statistical learning". www.web.stanford.edu/~hastie/ElemStatLearn/ (2009).

---

## 1.1 几则故事

我们将从一些短篇故事开始。每个故事都以夸张的手法展示了可解释的机器学习。如果你比较着急，你可以跳过它们。如果你想通过这几则故事对可解释的机器学习产生兴趣和动力，请继续读下去！

这种模式的灵感来自Jack Clark在他的[Import AI Newsletter](https://jack-clark.net/)讲述的技术故事。如果你喜欢这种故事，或者你对人工智能感兴趣，我建议你去浏览他的网站。

### 闪电绝不会击中两次

### 2030 年：瑞士医学实验室

![img](https://christophm.github.io/interpretable-ml-book/images/hospital.png)

“这绝对不是最糟糕的死法！“Tom总结了一下，他试图从悲剧中找到一些正面的东西。他从静脉点滴管上取下了输液泵。
“他是因为某些错误而死的，”Lena补充说。
“确切地说是用错了吗啡泵！这增加了我们的工作量！“Tom一边抱怨一边拧下泵的后板。拆下所有的螺丝后，他把盘子举起放在一边。他把数据线插入诊断端口。
“你不只是抱怨工作量，是吗？“Lena嘲笑他。“当然不是。从来没有！”他用讽刺的语气喊道。

他启动了连接着输液泵的计算机。
Lena把数据线的另一端插入平板电脑。“好吧，诊断正在运行。”她说，“我真的很好奇出了什么问题。”
“它确实把John Doe送入了天堂。这种吗啡浓度很高。嘿，伙计，我是说…这是第一次，对吧？通常情况下，一个坏的输液泵会释放出很少量的糖或者什么都没有。但是，你知道，从来没有像那样疯狂的注射，”Tom解释说。
“我知道。你不必说服我……嘿，看看那个。”Lena举起她的平板电脑。”你看到这里的峰值了吗？这就是止痛药混合物的功效。看！这一行显示的是参考水平。这个可怜的家伙在他的血液中混入了可以杀死他 17 次以上的止痛药。用我们的泵注射在这里。还有这里……”她侧身，“在这里你可以看到病人死亡的那一刻。”
“那么，知道发生了什么事吗，老板？“Tom问他的上司。
“嗯……传感器看起来不错。心率、含氧量、血糖等数据都是按预期收集的。血氧数据中有些缺失值，但这并不罕见。看这里，传感器还检测到病人心率减慢，以及吗啡衍生物和其他止痛药导致的极低的皮质醇水平<sub>a</sub>。”她继续浏览诊断报告。
Tom被屏幕迷住了。这是他第一次调查真正的设备故障。

“好吧，这是我们的第一个困惑。系统未能向医院的通信通道发送警告。警告已触发，但在协议层被拒绝了。可能是我们的错，但也可能是医院的错。请将日志发送给 IT 团队，”Lena告诉Tom。
Tom点了点头，眼睛仍然盯着屏幕。
Lena继续说：“真奇怪。警告也应导致输液泵关闭。但显然没有这样做。那一定是个 bug。质检团队一定漏检了什么东西，太糟糕了，这可能与应急方案有关。”
“所以，输液泵的应急系统不知怎么地坏了，但是为什么输液泵会如此疯狂地给John Doe注射那么多止痛药呢？“Tom问道。
“好问题！你是对的，除了应急方案发生故障外，输液泵不应该使用那么多的药物。考虑到皮质醇的含量较低和其他警告信号，该算法应该提前停止。“Lena解释道。
“也许是运气不好，比如百万分之一的运气，像被闪电击中？“Tom问她。“不，Tom，如果你阅读过我发给你的文件，你就会知道这个输液泵泵首先是在动物实验中测试的，然后是在人类身上测试的，以学习根据传感器输入来注射完美数量的止痛药。输液泵的算法可能是不透明且复杂的，但不是随机的。这意味着在同样的情况下，输液泵的工作方式会完全相同，可能会再次出现相同的病人死亡的情况。传感器输入的组合或不期望的交互作用一定触发了输液泵的错误行为。这就是为什么我们必须深入挖掘，找出这里发生了什么。”Lena解释说。

“我明白了……”，Tom回答，陷入沉思。“病人不是很快就会死吗？因为癌症还是什么？“
Lena边看分析报告边点头。
Tom站起来走到窗前。他向外看，眼睛盯着远处的一个点。
“也许机器帮了他一个忙，让他摆脱痛苦，不再受苦。也许它只是做了正确的事，就像闪电，一个好闪电。我的意思是就像彩票，但不是随机的。但出于某种原因，如果我是输液泵，我也会这样做的。”她最后抬起头看着他。
他一直在看外面的东西。
两人都沉默了一会儿。
Lena又低下头继续分析，“不，Tom。这是一个bug……只是一个该死的 bug。”

### 失去信任

### 2050 年：新加坡的地铁站

![img](https://christophm.github.io/interpretable-ml-book/images/access-denied.jpg)

她赶到璧山地铁站，按照她的想法，她已经在工作了，新的神经架构的测试现在应该已经完成了。她领导重新设计了政府的“个人纳税亲和力预测系统”，该系统预测一个人是否会向税务局隐瞒资金。她的团队想出了一个优雅的想法。如果成功，该系统不仅可以为税务局服务，还可以提供给其他系统，如反恐警报系统和工商登记等。有一天，政府甚至可以将这些预测纳入公民信用评分来评估一个人的可信度。这个评估会影响到每个人日常生活的方方面面，比如贷款或者取得新护照的时间。当她走下扶梯时，她想象着她们团队的系统与公民信用评分系统的整合会是怎样的。

她像往常一样在不减缓行走速度的情况下用手划过RFID读卡器。她的思绪被占据了，但是期望和现实的不一致在她的大脑中敲响了警钟。

太晚了！

她的鼻子撞在地铁站入口的大门上，紧接着屁股摔倒在地上。门本来应该会打开的，…但它没有。她十分惊讶地站了起来，看着大门旁边的屏幕。“请再试一次，”屏幕上一个友好的笑脸建议道。一个人从她身旁经过，没有半点关注她的意思，而是自顾自地将手从读卡器上划过。门开了，他走了进去，门又关上了。她擦了擦受伤但好在没有流血的鼻子，然后试图开门，但又被拒绝了。这很奇怪，也许她的公共交通账户没有足够的代币，这样想着，然后她用智能手表查看帐户余额。

“登录被拒绝，请联系您的公民咨询局！“她的手表告知她。

恶心的感觉像拳头打在她的肚子上，她怀疑是发生了什么事。为了证实她的想法，她启动了移动游戏“狙击手工会”，一个第一视角狙击手游戏。应用程序又自动关闭了，这证实了她的想法。她头晕目眩，再次坐在地板上。

只有一个可能的解释：她的公民信用分数下降了。基本上，小幅度的下降意味着一些小的不便，比如没有头等舱的航班，或者需要等待更长的时间才能获得官方文件。但过低的信用度分数是很少见的，这意味着你被列为对社会是有威胁的。对付这些人的一个措施是让他们远离公共场所，比如地铁站。政府限制了低信用分数公民的金融交易。政府也开始积极监控你在社交媒体上的行为，甚至限制某些内容，比如暴力游戏。公民信用分数越低，增加公民信用分的难度就成倍地增加，得分很低的人通常不会恢复原来分数。

她想不出任何理由为什么她的分数会下降。该分数基于机器学习，公民信用评分系统就像一台运转良好的发动机，为社会服务，信用评分系统的性能无时无刻不受到密切监控。自本世纪初以来，机器学习变得越来越好，它变得如此有效，以至于信用评分系统做出的决定不再有争议。它是一个可靠的系统。

她绝望地笑了。只要系统很少出故障它就是可靠的系统，但它现在出故障了。她一定是那些特殊情况中的一个，系统错误。从现在起，她就成了被放逐者。没有人敢质疑这个系统，它过于融入了政府，融入了社会，这是不可质疑的。在仅存的少数几个民主国家，禁止开展反对民主运动，不是因为这些国家在本质上是恶意的，而是因为它们会破坏现有制度的稳定。同样的逻辑也适用于现在更常见的算法，由于会对现状造成危险，禁止对算法进行批判。

算法信任是社会秩序的结构。为了共同的利益，少数的虚假信任评分被默默地接受，成百上千的其他预测系统和数据库的输入最终得出了这个分数，使得无法得知是什么导致了她的分数下降。她觉得自己身下有一个巨大的黑洞正在张开，她惊恐地望着虚空。

她的纳税亲和力系统最终被纳入公民信用评分系统，但她永远都不会知道了。

### 费米的回形针

### 火星定居612 年后：火星博物馆

![img](https://christophm.github.io/interpretable-ml-book/images/burnt-earth.jpg)

“历史好无聊啊，”Xola悄悄地对她的朋友说。Xola，一个蓝头发的女孩，正在房间里懒洋洋地用左手追着一架嗡嗡作响的投影无人机。“历史很重要，”老师看着女孩们，用一种沮丧的声音说道。Xola 脸红了，她没想到老师会听到了她说话。

“Xola，你刚学到了什么？”老师问她。“古人用尽了地球(Earther Planet)上所有的资源，然后死了？“她仔细地问。“不，不是人类让气候变热，而是电脑和机器。“这是行星地球(Planet Earth)，而不是地球行星(Earther Planet)<sub>b</sub>，”另一个叫Lin的女孩补充说。Xola点头表示同意。带着一丝骄傲，老师微笑着点了点头，说道：“你们都是对的，知道为什么会这样吗？”。“因为人们目光短浅，贪婪？”Xola 问道。“人们无法停止他们的机器！”Lin脱口而出。

“你们又一次都是对的，”老师说道，“但实际上比这复杂得多。当时大多数人都不知道发生了什么。有些人看到了巨大的变化，但无法逆转。这一时期最著名的作品是一首匿名作者的诗，它很好地描述了当时发生的事情。认真听！“

老师开始写这首诗。十几架小型无人机在孩子们面前重新定位，开始将视频直接投射到他们的眼睛里。影像中是一个穿着西装的人，他站在只剩下树桩的森林里，开始说话：

*机器计算；机器预测。*

*我们作为其中的一部分继续向前。*

*我们追逐着训练后的最优解。*

*它是一维的，是局部的，是无约束的。*

*硅片和肉体，呈指数型增长。*

*我们的灵魂在成长。*

*当得到所有奖励，*

*副作用却被忽略；*

*当开采出所有的硬币，*

*却忘记了已落在身后的自然；*

*麻烦终有一天会到来，*

*毕竟，指数增长终为泡影。*

*所有人的悲剧正在上演，*

*爆炸，*

*发生在我们眼前。*

*冰冷的计算与贪婪，*

*能让地球充满热血吗？*

*一切都在死亡，*

*而我们只能顺从。*

*我们像被蒙住眼睛的马匹争相创造，*

*向着伟大的文明大步向前。*

*我们坚持不懈地前进，*

*最终成为机器的一部分，*

*拥抱混乱的世界。*

“黑暗的记忆，”老师说道，打破了房间里的沉默。“这首诗将上传到你们的电子图书馆中，记得下个星期交作业。”Xola 叹了口气，她设法抓住了一只小无人机。无人机从中央处理器(CPU)和引擎释放温暖，Xola 喜欢用它温暖她的手。

- 译注a：过低的皮质醇含量可能意味着脑垂体及肾上腺的功能异常，是生命体征变弱的一种可能症状


- 译注b：这里是为了表现Xola不好好听课，实际上没有“Earther”这个词

---

## 1.2 什么是机器学习？

机器学习是计算机以学习数据特征为基础来进行预测及提升其表现的方法的总称。

例如，为了预测房子的价格，计算机将从过去的房屋销售中学习价格变动的模式。这本书重点关注有监督机器学习，它涵盖了在我们的房屋预测数据集上的所有预测问题，对于这个数据集，我们已经知道我们感兴趣的结果(如过去的房价)，并希望通过学习，预测新数据的结果。有一些任务是不包含在有监督学习中的，例如聚类任务是一种无监督学习，它们没有我们关注的特定结果，但我们希望能从中找到数据点的簇(中心点)。此外，还有一些方法不属于有监督学习，如强化学习，其中的代理结构通过在一个环境中(如计算机玩俄罗斯方块)的行为来学习强化某种激励。有监督学习的目的是学习一个预测模型，将数据特征(如房屋大小、位置、楼层类型等)映射到输出(如房价)。如果输出的是类别，则将这个任务称为分类；如果输出的是数值，则称之为回归。机器学习算法通过估计参数(如权重)或学习结构(如树结构)来学习模型，由一个分数或最小化的损失函数进行指导。在预测房屋价格的例子中，机器将最小化房子价格和预测价格之间的差值。然后，可以使用经过足够多训练的机器学习模型来预测新实例中房子的价格。

房价估计、产品推荐、路标检测、信贷违约预测和欺诈检测等例子都有共同点，都可以通过机器学习来解决。任务不同，但方法可以是相同的：

第一步：数据采集。数据越多越好，必须包含要预测的结果以及预测结果需要依赖的其他数据。对于路标检测器(“检测图像中是否有路标“)，需要收集街道图像并标记其中是否存在路标。对于信贷违约预测器，需要过去实际贷款的数据、客户是否拖欠贷款的信息以及对预测有帮助的数据，例如收入、过去的信贷违约等。对于自动房价估计程序，可以从过去的房屋销售中收集数据，还需要有关房地产的信息，如大小、位置等。

第二步：将这些信息输入机器学习算法，训练出路标检测器模型、信用评级模型或房价估计器。

第三步：将新数据输入模型。将模型集成到产品或流程中，例如自动驾驶汽车、信贷申请处理程序或房地产市场网站。

机器在许多任务上的表现都超过人类，例如下棋(最近的Alpha-go)或天气预测。即使机器和人的表现一样好，或者在某些任务上略微逊色，但在速度、重复性和可伸缩性等方面仍然有很大的优势。一个训练完成的机器学习模型可以比人类更快地完成一项任务，可靠地提供一致的结果，并且可以无限地复制。在另一台机器上复制机器学习模型既快又便宜。而训练一个人完成一项任务可能需要几十年(尤其是当他们年轻时)，并且成本很高。使用机器学习的一个主要缺点是，关于数据和机器解决任务的思路被隐藏在愈加复杂的模型中，你需要数以百万计的参数来描述一个深层的神经网络，却没有办法完全理解这个模型。在其他深度模型中也是类似的，如由数百个决策树组成的随机森林，通过这些决策树的“投票”进行预测。为了理解这个决定是如何做出的，必须查看这数百棵树中每一棵树的投票结果以及树的结构。不管你有多聪明或者记忆力有多好，这几乎都是不可能的。即使每个模型都可以被解释，但通常由多个模型融合(或组成)的性能最好的模型是不能被解释的。如果只关注性能，你就会获得越来越多不透明的模型。在机器学习竞赛平台上，胜出的模型大多是集成模型或非常复杂的模型，如增强树或深度神经网络。

---

## 1.3 术语

为避免歧义引起混淆，本书中使用的术语定义如下：

**算法**是机器为达到特定目标[$^2$](https://christophm.github.io/interpretable-ml-book/#fn2%20class=)而遵循的一组规则，可以将算法视为定义输入、输出以及从输入到输出所需的所有步骤的方法。算法就像是一种食谱，其中的输入是食材，输出的是美味的食物，准备和烹饪步骤就是算法指令。

**机器学习**是所有使得计算机可以学习数据特征以做出和改进预测(如癌症、每周销售额、信贷违约等)方法的总称。机器学习是从“正常编程”到“间接编程”的一种范式转换，在“直接编程”中所有指令都必须明确地提供给计算机，而“间接编程”是通过提供数据来实现的。

![img](https://christophm.github.io/interpretable-ml-book/images/programing-ml.png)

**学习机**或**机器学习算法**是用来从数据中学习机器学习模型的程序也可以称之为“诱导器”(如“树诱导器”)。

**机器学习模型**是学习到的将输入映射到预测的程序，它可以是线性模型或神经网络的一组权重。“模型”其实是一个不太具体的词，它还有一些其他的名字如“预测器”，或者可以根据任务类型称之为“分类器”或者“回归模型”。在公式中，经过训练的机器学习模型用符号$\hat{f}$或 $\hat{f}(x)$表示。

![img](https://christophm.github.io/interpretable-ml-book/images/learner.png)

<center><font size = 2>图1.1 学习者从标记的训练数据学习用于预测的模型</font></center>

**黑盒模型**是一个不展示其内部机制的系统。在机器学习中，“黑盒”用来描述仅通过查看其参数(如神经网络)无法被理解的模型。与黑盒相对的模型有时被称为**白盒**，在本书中被称为[自解释性模型](https://christophm.github.io/interpretable-ml-book/simple.html#simple)。无论一个机器学习模型是否是黑盒，用于研究可解释性的[模型无关方法](https://christophm.github.io/interpretable-ml-book/agnostic.html#agnostic)统一将其视为黑盒。

![img](https://christophm.github.io/interpretable-ml-book/images/iml.png)

**可解释的机器学习**是指使机器学习系统的行为和预测可被人类理解的方法和模型。

**数据集**是一个包含可供机器从中学习的数据的列表。数据集包含样例特征及需要被预测的目标。当被用于训练模型时，数据集称为训练数据。

**样例**是数据集中的一行，也可称之为(数据)点，样本，观测点等。实例由特征值 $x^{(i)}$ 和目标结果 $y_i$(如果结果已知的话)组成。

**特征**是用于进行预测或分类的输入，是数据集中的列。在本书中，假定特征都是可解释的，也就是说很容易理解它们的含义，比如某一天的温度或一个人的身高。特征的可解释性是一个很大的假设，但是如果很难理解输入特征的话，那么将更难理解模型的行为。包含所有特征的矩阵用$X$表示，单个样例的特征用$x^{(i)}$表示。所有实例的单个特征向量是$x_j$，而实例 $i$ 的特征$j$用$x^{(i)}_j$来表示。

**目标值**是机器要通过学习来进行预测的信息。对于单个实例，目标通常用$y$或$y_i$来表示。

**机器学习任务**是一个具有特征和目标值的数据集的组合。根据目标值的类型，任务可以被分为分类、回归、生存分析、聚类或异常值检测。

**预测**是机器学习模型根据给定的特征“猜测”目标值应该是什么的过程。在本书中，模型预测用 $\hat{f}(x^{(i)})$  或 $\hat{y}$ 表示。

2. "Definition of Algorithm." https://www.merriam-webster.com/dictionary/algorithm. (2017).

